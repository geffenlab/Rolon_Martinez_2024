{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 44em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))\n",
    "display(HTML(\"<style>div.output_scroll { height: 44em; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import functions you will need for running this script\n",
    "# %matplotlib widget\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "import matplotlib.lines as mlines\n",
    "from scipy import io\n",
    "from scipy import signal\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy import stats\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to load events and spikes:\n",
    "def get_events(dataLoc):\n",
    "    # Attempt to load required files and handle exceptions if files are missing or corrupted\n",
    "    try:\n",
    "        msgText = np.load(os.path.join(dataLoc, 'messages', 'text.npy'))\n",
    "        msgSample = np.load(os.path.join(dataLoc, 'messages', 'timestamps.npy'))\n",
    "        evState = np.load(os.path.join(dataLoc, 'events', 'channel_states.npy'))\n",
    "        evSample = np.load(os.path.join(dataLoc, 'events', 'timestamps.npy'))\n",
    "        recInfo = np.genfromtxt(os.path.join(dataLoc, 'sync_messages.txt'), dtype='str', skip_header=1, delimiter=' ')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading files: {e}\")\n",
    "        return None\n",
    "\n",
    "    recInfo = recInfo[-1]\n",
    "    startTime, fs = int(recInfo.split('@')[0]), int(recInfo.split('@')[1][:-2])\n",
    "\n",
    "    evTS = (evSample - startTime) / fs\n",
    "    msgTS = (msgSample - startTime) / fs\n",
    "\n",
    "    # Print each message text on a new line with its index, decoding from byte-string\n",
    "    for index, text in enumerate(msgText):\n",
    "        print(f\"{index}: {text.decode('utf-8')}\")  # Decoding from UTF-8\n",
    "        \n",
    "    print(\"Total number of blocks:\", len(msgTS))\n",
    "\n",
    "    blockStartID = int(input(\"Indicate Block Start here (remember python syntax were 0 = 1):\"))\n",
    "    blockStart = msgTS[blockStartID]\n",
    "    blockEnd = msgTS[blockStartID + 1] if blockStartID + 1 < len(msgTS) else None\n",
    "  \n",
    "    if blockEnd is None:\n",
    "        blockEvTS = evTS[evTS > blockStart]\n",
    "        blockEvState = evState[evTS > blockStart]\n",
    "    else:\n",
    "        blockEvTS = evTS[(evTS > blockStart) & (evTS < blockEnd)]\n",
    "        blockEvState = evState[(evTS > blockStart) & (evTS < blockEnd)]\n",
    "\n",
    "    stimulusOn = blockEvTS[blockEvState == 1][1:]\n",
    "    stimulusOff = blockEvTS[blockEvState == -1][1:]\n",
    "    \n",
    "\n",
    "    #spike extraction events are done, now load spike data\n",
    "    spikes = np.load(os.path.join(dataLoc, 'spike_times.npy'))/ fs\n",
    "    clust = np.load(os.path.join(dataLoc, 'spike_clusters.npy'))\n",
    "       \n",
    "    #   load mean waveforms and get cluster depth\n",
    "    mean_waves = loadmat(os.path.join(dataLoc,'mean_waveforms.mat'), appendmat=True)\n",
    "    clust_Depth = mean_waves['chanMap']['ycoords'][0][0][0][np.squeeze(mean_waves['mn'])-1]\n",
    "    clust_Depth = np.int32(clust_Depth) - np.int32(np.asarray(np.max(mean_waves['chanMap']['ycoords'][0][0][0])))\n",
    "  \n",
    "    # load cluster information\n",
    "    clustInfo = pd.read_csv(os.path.join(dataLoc, 'cluster_info.tsv'),delimiter='\\t')\n",
    " \n",
    "\n",
    "    # remove noise clusters\n",
    "    clust_Depth = clust_Depth[clustInfo.group != 'noise']\n",
    "    clustInfo = clustInfo[clustInfo.group != 'noise']\n",
    "    \n",
    "    \n",
    "    # extract the cell index\n",
    "    clustID = np.asarray(clustInfo.id)\n",
    "    \n",
    "    return evState, evTS, msgTS, blockEvTS, stimulusOn, spikes, clust, clust_Depth, clustInfo, stimulusOff\n",
    "    # return evState, evTS, msgTS, blockEvTS, stimulusOn, spikes, clust,clustInfo, stimulusOff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load stimulus information. It requires 2 inputs, 1) your file location & 2) the name of your file\n",
    "def stimuli(stim_Loc, name):\n",
    "    os.chdir(stim_Loc)\n",
    "    file = loadmat(name, appendmat=True) #loads the .m file with your stimuli\n",
    "    stimInf = file['stimInfo'] #extracts stimulus information\n",
    "    val = stimInf[0,0]\n",
    "    stimInfo = dict()\n",
    "    stimInfo['trialOrder'] = val['trialOrder'] #extracts the trial order\n",
    "    stimInfo['tDur'] = val['tDur'] #extracts the tone duration\n",
    "    stimInfo['ITI'] = val['ITI'] #extracts the inter trial interval\n",
    "    stimInfo['laserDur'] = val['laserDur'] #extracts laser duration\n",
    "    return stimInfo, name #returns a dictionary with all your necessary stimulus information\n",
    "\n",
    "def laser_stimuli(stim_Loc, name):\n",
    "    os.chdir(stim_Loc)\n",
    "    file = loadmat(name, appendmat=True) #loads the .m file with your stimuli\n",
    "    stimInf = file['stimInfo'] #extracts stimulus information\n",
    "    val = stimInf[0,0]\n",
    "    stimInfo = dict()\n",
    "    stimInfo['ITI'] = val['ITI'] #extracts the inter trial interval\n",
    "    stimInfo['laserDur'] = val['laserDur'] #extracts laser duration\n",
    "    return stimInfo, name #returns a dictionary with all your necessary stimulus information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies a smoothing guassian filter to your data\n",
    "# This function applies a smoothing guassian filter to your data\n",
    "def SmoothGauss(X, M):\n",
    "    if X.ndim != 1:\n",
    "        raise ValueError(\"X must be a 1D array.\")\n",
    "    \n",
    "    sigma = M ** 0.5\n",
    "    G = np.arange(-M, M+1)\n",
    "    F = np.exp(-G**2 / ((2 * sigma)**2))\n",
    "    F /= np.sum(F)\n",
    "    Y = np.convolve(X, F, mode='full')\n",
    "    \n",
    "    start_index = M\n",
    "    end_index = start_index + len(X)\n",
    "    Y = Y[start_index:end_index]\n",
    "    \n",
    "    correction_start = np.sum(F) / (np.sum(F[:M]) + np.cumsum(F[M:M*2]))\n",
    "    correction_end = np.sum(F) / (np.sum(F[:M]) + np.cumsum(F[M:M*2])[::-1])\n",
    "    \n",
    "    Y[:M] *= correction_start\n",
    "    Y[-M:] *= correction_end\n",
    "    \n",
    "    return Y\n",
    "\n",
    "# This function extracts spikes and returns your raster, trials and psth variable:\n",
    "def spike_data(spikes, stimulusOnset, edges, smVar):\n",
    "    raster = []\n",
    "    trials = []\n",
    "    psth = np.empty(([len(stimulusOnset), len(edges)-1]))\n",
    "    psth_S = np.empty(([len(stimulusOnset), len(edges)-1]))\n",
    "    \n",
    "    for s, stim in enumerate (stimulusOnset):\n",
    "        \n",
    "        spks = spikes - stim\n",
    "        \n",
    "        psth[s,:], _ = np.histogram(spks, bins=edges)\n",
    "        psth[s,:] = psth[s,:]/np.diff(edges).mean()\n",
    "        psth_S[s,:] = SmoothGauss(psth[s,:],smVar)\n",
    "        \n",
    "        spks = spks[(spks > edges[0]) & (spks < edges[-1])]\n",
    "        raster.extend(spks)\n",
    "        trials.extend(np.ones(len(spks))*(s+1))\n",
    "        \n",
    "    return raster, trials, psth, psth_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ismember(A, B):\n",
    "    A = np.asarray(A).astype(int)\n",
    "    B = np.asarray(B).astype(int)\n",
    "    res = np.zeros(A.shape)\n",
    "    for i in np.unique(A):\n",
    "        res[A == i] = np.argwhere(B == i).squeeze()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/solymarrolon/Data/OpenEphys/MGB_Recordings/' #data folder\n",
    "data_paper = 'Data_Paper'\n",
    "\n",
    "# cellType = 'PV_Recordings' \n",
    "cellType = 'SOM_Recordings' \n",
    "\n",
    "\n",
    "# Virus = 'retro_stGtACR2'\n",
    "# Virus = 'stGtACR1'\n",
    "# Virus = 'ChR2'\n",
    "Virus = 'Controls' \n",
    "\n",
    "\n",
    "# mouse = ['S095','S097','S098'] #PV Mouse List_Chr2\n",
    "\n",
    "# mouse = ['S0157','S0158','S0163','S0164'] #PV Mouse List\n",
    "# mouse = ['S0154','S0155','S0161','S0162']  #SOM Mouse List\n",
    "\n",
    "# mouse = ['S0169','S0170'] #PV Mouse List Controls\n",
    "mouse = ['S0171','S0172'] #SOM Mouse List Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stim_loc = '/Users/solymarrolon/Data/Stimuli/07302020/' #Stimulus folder location chr2\n",
    "stim_loc = '/Users/solymarrolon/Data/Stimuli/02232022/' #Stimulus folder location\n",
    "\n",
    "# stim = \"TuningCurve_50ms_Laser_50ms_Frequencies3_80Hz_073020_stimInfo\" #Stimulus name chr2\n",
    "stim = \"TuningCurve_50ms_Laser_50ms_Frequencies3_80Hz_02232022_stimInfo\" #Stimulus name \n",
    "\n",
    "\n",
    "StimInfo, StimName = stimuli(stim_loc, stim) # This function will extract the stimulus information needed\n",
    "\n",
    "\n",
    "nreps = 2 #number of stimulus repetitions in the GUI\n",
    "trialOrder = np.matlib.repmat(StimInfo['trialOrder'],nreps,1) # Order of trials in the stimulus presented\n",
    "ITI = StimInfo['ITI'] # Interstimulus Time Interval \n",
    "laserDur = StimInfo['laserDur'] #Laser Duration \n",
    "laserStart = 0 #laser start time\n",
    "tDur = StimInfo['tDur'] # tone duration \n",
    "tStart = 0 #tone start time\n",
    "\n",
    "\n",
    "binSize = .002 #bin size for your spikes\n",
    "\n",
    "edges = np.arange(-.050,.20, binSize) #this will establish what spikes to choose and how small your bin size\n",
    "time = edges[:-1] # time vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique frequencies and stimulus index \n",
    "uniq_Freq, stim_Ind= np.unique(trialOrder[:,0], return_inverse=True)\n",
    "laserCond, laser_Ind= np.unique(trialOrder[:,1], return_inverse=True)\n",
    "\n",
    "# extract laser on and laser off trials\n",
    "laserOn = np.array(np.where(trialOrder[:,1] == 1)).squeeze()\n",
    "laserOff = np.array(np.where(trialOrder[:,1] == 0)).squeeze()\n",
    "\n",
    "sortI = np.lexsort((trialOrder[:,0],trialOrder[:,1]))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: laser only 10ms -10 reps\n",
      "1: laser only 25ms -10 reps\n",
      "2: laser only 50ms -10 reps\n",
      "3: laser only 100ms -10 reps\n",
      "4: tc 25ms tone 25ms laser - 2 reps\n",
      "5: tc 50ms tone 50ms laser - 2 reps\n",
      "6: tc 100ms tone 100ms laser - 2 reps\n",
      "7: attenuated tc 25ms tone 25ms laser - 1 reps\n",
      "8: attenuated tc 100ms tone 100ms laser - 1 reps\n",
      "Total number of blocks: 9\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Indicate Block Start here (remember python syntax were 0 = 1): 5\n"
     ]
    }
   ],
   "source": [
    "##### Setup directories and date format\n",
    "date = datetime.now().strftime('%m%d%Y')\n",
    "data_folder = 'PSTH/'\n",
    "psth_folder_name = f'PSTH_{StimName}_{date}'\n",
    "psth_dir = os.path.join(data, cellType, Virus, data_paper, data_folder, psth_folder_name)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(psth_dir):\n",
    "    os.makedirs(psth_dir)\n",
    "\n",
    "\n",
    "smoothingVariable = 2\n",
    "\n",
    "# Processing each mouse and session\n",
    "for mID in range(len(mouse)):\n",
    "    mouseList = os.path.join(data, cellType, Virus, data_paper, mouse[mID])\n",
    "    _, dirs, _ = next(os.walk(mouseList), ([], [], []))\n",
    "    dirs = sorted(dirs)\n",
    "\n",
    "    for sessionID in range(len(dirs)):\n",
    "        clear_output(wait=True)\n",
    "        dat = os.path.join(mouseList, dirs[sessionID], 'data')\n",
    "        evState, evTS, msgTS, blockEvts, stimOn, spikes, clust, clust_Depth, clustInfo, stimOff = get_events(dat)\n",
    "        \n",
    "        # Check if there are enough stimulus events\n",
    "        if len(stimOn) > 760:\n",
    "            evOn = evTS[evState == 1]\n",
    "            dOn = np.diff(evOn)\n",
    "            blockI = np.argwhere(np.abs(dOn - 5) < .1)\n",
    "            t = evTS[(evTS >= evOn[blockI[5]]) & (evTS < evOn[blockI[6]])]\n",
    "            w = evState[(evTS >= evOn[blockI[5]]) & (evTS < evOn[blockI[6]])]\n",
    "            newstimOn = t[w == 1][1:]  # Adjust index to skip the first element\n",
    "\n",
    "            cellID = np.asarray(clustInfo.id)\n",
    "            allPSTH = np.empty([len(newstimOn), len(edges)-1, len(cellID)])\n",
    "            allPSTH_S = np.empty([len(newstimOn), len(edges)-1, len(cellID)])\n",
    "            RastersAll, TrialsAll, spikeSortIAll = [], [], []\n",
    "\n",
    "            for i, ID in enumerate(cellID):\n",
    "                cSpks = spikes[clust == ID]\n",
    "                Rasters, Trials, allPSTH[:, :, i], allPSTH_S[:, :, i] = spike_data(cSpks, newstimOn, edges, smoothingVariable)\n",
    "                spikeSortI = ismember(Trials, sortI)\n",
    "                RastersAll.append(Rasters)\n",
    "                TrialsAll.append(Trials)\n",
    "                spikeSortIAll.append(spikeSortI)\n",
    "\n",
    "            psth_name = f'PSTH for {mouse[mID]}_{dirs[sessionID]} {StimName}'\n",
    "            np.savez_compressed(os.path.join(psth_dir, psth_name),\n",
    "                                allPSTH=allPSTH,\n",
    "                                allPSTH_S=allPSTH_S,\n",
    "                                clustDepth=np.array(clust_Depth, dtype=object),  # If clust_Depth is a list of arrays of varying sizes\n",
    "                                rasters=np.array(RastersAll, dtype=object),  # If RastersAll contains arrays of different shapes\n",
    "                                trials=np.array(TrialsAll, dtype=object),  # If TrialsAll contains arrays of different shapes\n",
    "                                spikeSortI=np.array(spikeSortIAll, dtype=object))  # If spikeSortIAll contains arrays of different shapes\n",
    "        else:\n",
    "            # Handle cases with insufficient stimulus events similarly\n",
    "            cellID = np.asarray(clustInfo.id)\n",
    "            allPSTH = np.empty([len(stimOn), len(edges)-1, len(cellID)])\n",
    "            allPSTH_S = np.empty([len(stimOn), len(edges)-1, len(cellID)])\n",
    "            RastersAll, TrialsAll, spikeSortIAll = [], [], []\n",
    "\n",
    "            for i, ID in enumerate(cellID):\n",
    "                cSpks = spikes[clust == ID]\n",
    "                Rasters, Trials, allPSTH[:, :, i], allPSTH_S[:, :, i] = spike_data(cSpks, stimOn, edges, smoothingVariable)\n",
    "                spikeSortI = ismember(Trials, sortI)\n",
    "                RastersAll.append(Rasters)\n",
    "                TrialsAll.append(Trials)\n",
    "                spikeSortIAll.append(spikeSortI)\n",
    "\n",
    "            psth_name = f'PSTH for {mouse[mID]}_{dirs[sessionID]} {StimName}'\n",
    "            np.savez_compressed(os.path.join(psth_dir, psth_name),\n",
    "                                allPSTH=allPSTH,\n",
    "                                allPSTH_S=allPSTH_S,\n",
    "                                clustDepth=np.array(clust_Depth, dtype=object),  # If clust_Depth is a list of arrays of varying sizes\n",
    "                                rasters=np.array(RastersAll, dtype=object),  # If RastersAll contains arrays of different shapes\n",
    "                                trials=np.array(TrialsAll, dtype=object),  # If TrialsAll contains arrays of different shapes\n",
    "                                spikeSortI=np.array(spikeSortIAll, dtype=object))  # If spikeSortIAll contains arrays of different shapes\n",
    "# Note: Additional error handling and logging would enhance the robustness and traceability of this script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
